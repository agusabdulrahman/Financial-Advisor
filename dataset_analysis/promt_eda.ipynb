{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Exploratory Data Analysis\n",
    "Using this notebook, we can analyze and extract key insight from the promt used for trainig our LLM model.\n",
    "\n",
    "#### Eda Overview\n",
    "Token Wise:\n",
    "- Average Tokens Lengt\n",
    "- Token Lengths Distribution\n",
    "- Min/Max in tokes \n",
    "\n",
    "Coverage:\n",
    "- Promt Coverage as a funtion of tokens length\n",
    "- Promt coverage as a funtion of hardcoded LLM context size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\agusa\\appdata\\roaming\\python\\python311\\site-packages (3.7.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (2.14.7)\n",
      "Requirement already satisfied: transformers in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (4.32.1)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.3-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from seaborn) (1.11.4)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\agusa\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\agusa\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\agusa\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\agusa\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\agusa\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (0.19.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas>=0.23->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\agusa\\anaconda3\\envs\\tf\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Downloading wordcloud-1.9.3-cp311-cp311-win_amd64.whl (300 kB)\n",
      "   ---------------------------------------- 0.0/300.2 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.2 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/300.2 kB 279.3 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 71.7/300.2 kB 326.8 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 122.9/300.2 kB 423.5 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 194.6/300.2 kB 535.8 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 225.3/300.2 kB 550.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 276.5/300.2 kB 472.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 300.2/300.2 kB 475.3 kB/s eta 0:00:00\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.3\n"
     ]
    }
   ],
   "source": [
    "# Setup Dependencies\n",
    "! pip install seaborn matplotlib datasets transformers wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agusa\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 349kB/s]\n",
      "c:\\Users\\agusa\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\agusa\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:01<00:00, 193kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 493kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer : BertTokenizerFast(name_or_path='sentence-transformers/all-MiniLM-L6-v2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': 'EOS', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': 'UNK', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "tokenizer.eos_token = \"EOS\"\n",
    "tokenizer.pad_token = \"UNK\"\n",
    "print(f\"Loaded tokenizer : {tokenizer}\" if tokenizer else \"Failed to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data parser\n",
    "from enum import Enum \n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "class Scope(Enum):\n",
    "    TRAIN = \"train\"\n",
    "    \n",
    "@dataclass\n",
    "class DataSample:\n",
    "    about_me: str = field(repr=False)\n",
    "    context: str = \"\"\n",
    "    response: str = \"\"\n",
    "    \n",
    "    def formatted(self):\n",
    "        \"\"\"\"Train containing raw text fields\"\"\"\n",
    "        template = \"\"\"\n",
    "        {ABOUT}\n",
    "        {CONTEXT}\n",
    "        {RESPONSE}\n",
    "        \"\"\"\n",
    "        complete = template.format(ABOUT=self.about_me, CONTEXT=self.context, RESPONSE=self.response)\n",
    "        return complete\n",
    "\n",
    "class DatasetsGenerator:\n",
    "    def __init__(self, dataset_file_path: Path, scope: Scope):\n",
    "        self._scope = scope\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataser = self._load_dataset(dataset_file_path)\n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.dataset[self._scope.value])  \n",
    "    \n",
    "    def _load_dataset(self, dataset_file_path: Path):\n",
    "        try:\n",
    "            dataset = load_dataset(\"json\", data_files=str(dataset_file_path))\n",
    "            print(\"=\", * 10 + \" info \" + \"=\" * 10)\n",
    "            print(f\"Dataset Path : {dataset_file_path}\")\n",
    "            print(f\"Loaded : {len(dataset[self._scope.value])}\") \n",
    "        except FileNotFoundError:\n",
    "            print(\"JSON file not found.\")\n",
    "        except SyntaxError:\n",
    "            print(f\"JOSN file badly formateed.\")\n",
    "            raise FileNotFoundError(\"Forgot how to write json\")\n",
    "        \n",
    "    def tokenized_sample(self, tokenizer):\n",
    "        formatted_samples = list(map(lambda en: DataSample(**en), self.dataset[self._scope.value]))\n",
    "        tokenized_samples = []\n",
    "        for sample in formatted_samples:\n",
    "            as_str = sample.formatted()\n",
    "            tokenized = tokenizer(as_str)\n",
    "            tokenized = tokenized[\"input_ids\"] \n",
    "            tokenized_samples.append(tokenized)\n",
    "        return tokenized_samples\n",
    "    \n",
    "    def samples_gen(self):\n",
    "        formatted_samples = list(map(lambda en: DataSample(**en), self.dataset[self._scope.value]))\n",
    "        for sample in formatted_samples:\n",
    "            yield sample              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file not found.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetsGenerator' object has no attribute 'tokenized_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 2. Prepare dataset\u001b[39;00m\n\u001b[0;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m DatasetsGenerator(dataset_file_path\u001b[38;5;241m=\u001b[39mdataset_json_file, scope\u001b[38;5;241m=\u001b[39mScope\u001b[38;5;241m.\u001b[39mTRAIN)\n\u001b[1;32m----> 7\u001b[0m tokenized_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenized_samples\u001b[49m(tokenizer)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DatasetsGenerator' object has no attribute 'tokenized_samples'"
     ]
    }
   ],
   "source": [
    "# 1. Prerequisites\n",
    "DATASET_ROOT = Path(\"../modules/q_and_a_dataset_generator/data\")\n",
    "dataset_json_file = DATASET_ROOT / \"training_data.json\"\n",
    "\n",
    "# 2. Prepare dataset\n",
    "dataset = DatasetsGenerator(dataset_file_path=dataset_json_file, scope=Scope.TRAIN)\n",
    "tokenized_samples = dataset.tokenized_samples(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../modules/q_and_a_dataset_generator/data/training_data.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
